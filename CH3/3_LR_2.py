#coding=utf-8

import torch
import matplotlib.pyplot as plt
from time import perf_counter

#å‡†å¤‡æ•°æ®

#ç”ŸæˆçŸ©é˜µX
def Produce_X(x):
	x0 = torch.ones(x.numpy().size) #ç”¨onesäº§ç”Ÿåˆå§‹å€¼ä¸º1ï¼Œå¤§å°ä¸xç›¸åŒçš„å‘é‡
	X = torch.stack((x,x0),dim=1)   #stackå‡½æ•°å°†ä¸¤ä¸ªå‘é‡æ‹¼åˆ
	print(X)
	return X


x = torch.linspace(-3,3,100000)#ç”¨linspaceäº§ç”Ÿï¼ˆ-3ï¼Œ3ï¼‰åŒºé—´å†…çš„100000ä¸ªç‚¹
X = Produce_X(x)
y = x +1.2*torch.rand(x.size())#å‡è®¾çœŸå®å‡½æ•°æ˜¯y=xï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢å¢åŠ ä¸€äº›è¯¯å·®ï¼Œæ›´åŠ ç¬¦åˆå®é™…æƒ…å†µ
w = torch.rand(2) #å®šä¹‰æƒé‡wçš„å˜é‡
print(w)

'''
#æ•£ç‚¹å›¾æŸ¥çœ‹æ ·æœ¬æ•°æ®çš„åˆ†å¸ƒæƒ…å†µ
plt.scatter(x.numpy(),y.numpy(),s=0.001)
plt.show()
'''

#å¦‚æœæ”¯æŒCUDAï¼Œåˆ™é‡‡ç”¨CUDAåŠ é€Ÿ
CUDA =  torch.cuda.is_available()

if CUDA:
	inputs = X.cuda() 
	target = y.cuda()
	w = w.cuda()
	w.requires_grad=True
else:
	inputs = X 
	target = y
	w = w
	w.requires_grad=True

#å¯è§†åŒ–
#plt.ion() 
#plt.show()

def draw(output,loss):
	#print loss
	if CUDA:
		output= output.cpu()
	plt.cla()
	plt.scatter(x.numpy(), y.numpy())
	plt.plot(x.numpy(), output.data.numpy(),'r-', lw=5)
	plt.text(0.5,0,'Loss=%s' % (loss.item()),fontdict={'size':20,'color':'red'})
	plt.pause(0.005)#ç»˜å›¾å»¶è¿Ÿï¼Œå¹¶ä¸”ä¿ç•™ä¹‹å‰æ‰€åŒ–çš„å›¾åƒ

def train(epochs=1,learning_rate=0.01):
	for epoch in range(epochs):
	
		#å‰å‘ä¼ æ’­
		output = inputs.mv(w) #å…¬å¼ï¼šy=Xw
		loss = (output - target).pow(2).sum()/100000 #å…¬å¼ï¼šJ = (âˆ‘(y-y')^2)/100000

		#åå‘ä¼ æ’­
		loss.backward() 
		w.data -= learning_rate * w.grad  #æ›´æ–°æƒé‡wï¼Œå…¬å¼ï¼šw_(t+1)= w_(t) - ğœ¼*â–½J
		w.grad.zero_() #æ¸…ç©ºgradçš„å€¼
		
		
		if epoch % 80 == 0:
			draw(output,loss)
		

	return w,loss

start = perf_counter()
w,loss = train(10000,learning_rate=1e-4)  #å­¦ä¹ ç‡è®¾ç½®ä¸º1x10^(-4)
finish = perf_counter()
time = finish-start

print("è®¡ç®—æ—¶é—´:%s" % time)
print("final loss:",loss.item())
print("weights:",w.data)
